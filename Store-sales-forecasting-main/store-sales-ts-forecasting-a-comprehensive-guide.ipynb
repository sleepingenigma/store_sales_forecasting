{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/29781/logos/header.png?t=2021-09-22-19-59-35)\n\n# Store Sales - Time Series Forecasting\n\nBefore reading the notebook, what will you learn from this notebook?\n\n- Interpolation for Oil Prices\n- Detailed Data Manipulation for Holiday and Events Data\n- Exploratory Data Analysis\n- Hypothesis Testing\n- Modelling","metadata":{}},{"cell_type":"markdown","source":"This competition is about time series forcasting for store sales. The data comes from an Ecuador company as known as Corporaci√≥n Favorita and it is a large grocery retailer. Also, the company operates in other countries in South America.\n\nIf you wonder the company, you can click [**here**](https://www.corporacionfavorita.com/en/) to learn something about it.\n\n<br>\n\n<center><img src=\"https://github.com/EkremBayar/Kaggle/blob/main/Images/CF.png?raw=true\" style=\"width:30%;height:10%;\"></center>\n\n<center><img src=\"https://github.com/EkremBayar/Kaggle/blob/main/Images/CF1.jpg?raw=true\" style=\"width:30%;height:10%;\"></center>","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:33:01.5483Z","iopub.execute_input":"2021-10-21T17:33:01.549087Z","iopub.status.idle":"2021-10-21T17:33:01.554094Z","shell.execute_reply.started":"2021-10-21T17:33:01.549052Z","shell.execute_reply":"2021-10-21T17:33:01.55281Z"}}},{"cell_type":"markdown","source":"There are **54 stores** and **33 prodcut families** in the data. The time serie starts from **2013-01-01** and finishes in **2017-08-31**. However, you know that Kaggle gives us splitted two data as train and test. The dates in the test data are for the **15 days** after the last date in the training data. Date range in the test data will be very important to us while we are defining a cross-validation strategy and creating new features.\n\n***Our main mission in this competition is, predicting sales for each product family and store combinations.***\n\nThere are 6 data that we will study on them step by step.\n1. *Train*\n2. *Test*\n3. *Store*\n4. *Transactions* \n5. *Holidays and Events*\n6. *Daily Oil Price*\n\n**<code>The train data</code>** contains time series of the stores and the product families combination. The sales column gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).The onpromotion column gives the total number of items in a product family that were being promoted at a store at a given date.\n\n**<code>Stores data</code>** gives some information about stores such as city, state, type, cluster.\n\n**<code>Transaction data</code>** is highly correlated with train's sales column. You can understand the sales patterns of the stores.\n\n**<code>Holidays and events data</code>** is a meta data. This data is quite valuable to understand past sales, trend and seasonality components. However, it needs to be arranged. You are going to find a comprehensive data manipulation for this data. That part will be one of the most important chapter in this notebook.\n\n**<code>Daily Oil Price data</code>** is another data which will help us. Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices. That's why, it will help us to understand which product families affected in positive or negative way by oil price.\n\n#### When you look at the data description, you will see \"Additional Notes\". These notes may be significant to catch some patterns or anomalies. I'm sharing them with you to remember.\n- Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.\n- A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.\n\n<center><h2> Let's start! </h2> </center>\n\n<center><img src=\"https://media.istockphoto.com/photos/flag-and-church-in-guayaquil-picture-id481766414?k=20&m=481766414&s=612x612&w=0&h=s8CFr9trtS6Dc3XlecsV1yTzw4FrUQR97ScDbym33jc=\" style=\"width:70%;height:10%;\"></center>\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Packages\n\nYou can find the packages below what I used.","metadata":{}},{"cell_type":"code","source":"# BASE\n# ------------------------------------------------------\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport warnings\n\n# PACF - ACF\n# ------------------------------------------------------\nimport statsmodels.api as sm\n\n# DATA VISUALIZATION\n# ------------------------------------------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n# CONFIGURATIONS\n# ------------------------------------------------------\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:29.820764Z","iopub.execute_input":"2021-10-30T20:14:29.821209Z","iopub.status.idle":"2021-10-30T20:14:34.091683Z","shell.execute_reply.started":"2021-10-30T20:14:29.821165Z","shell.execute_reply":"2021-10-30T20:14:34.090317Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Importing Data","metadata":{}},{"cell_type":"code","source":"# Import\ntrain = pd.read_csv(\"../input/store-sales-time-series-forecasting/train.csv\")\ntest = pd.read_csv(\"../input/store-sales-time-series-forecasting/test.csv\")\nstores = pd.read_csv(\"../input/store-sales-time-series-forecasting/stores.csv\")\n#sub = pd.read_csv(\"../input/store-sales-time-series-forecasting/sample_submission.csv\")   \ntransactions = pd.read_csv(\"../input/store-sales-time-series-forecasting/transactions.csv\").sort_values([\"store_nbr\", \"date\"])\n\n\n# Datetime\ntrain[\"date\"] = pd.to_datetime(train.date)\ntest[\"date\"] = pd.to_datetime(test.date)\ntransactions[\"date\"] = pd.to_datetime(transactions.date)\n\n# Data types\ntrain.onpromotion = train.onpromotion.astype(\"float16\")\ntrain.sales = train.sales.astype(\"float32\")\nstores.cluster = stores.cluster.astype(\"int8\")\n\ntrain.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:34.096472Z","iopub.execute_input":"2021-10-30T20:14:34.097207Z","iopub.status.idle":"2021-10-30T20:14:38.95747Z","shell.execute_reply.started":"2021-10-30T20:14:34.09715Z","shell.execute_reply":"2021-10-30T20:14:38.955639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Transactions\n\n**Let's start with the transaction data**","metadata":{}},{"cell_type":"code","source":"transactions.head(10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:38.959891Z","iopub.execute_input":"2021-10-30T20:14:38.96036Z","iopub.status.idle":"2021-10-30T20:14:38.978796Z","shell.execute_reply.started":"2021-10-30T20:14:38.960314Z","shell.execute_reply":"2021-10-30T20:14:38.976904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This feature is highly correlated with sales but first, you are supposed to sum the sales feature to find relationship. Transactions means how many people came to the store or how many invoices created in a day.\n\nSales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n\nThat's why, transactions will be one of the relevant features in the model. In the following sections, we will generate new features by using transactions.","metadata":{}},{"cell_type":"code","source":"temp = pd.merge(train.groupby([\"date\", \"store_nbr\"]).sales.sum().reset_index(), transactions, how = \"left\")\nprint(\"Spearman Correlation between Total Sales and Transactions: {:,.4f}\".format(temp.corr(\"spearman\").sales.loc[\"transactions\"]))\npx.line(transactions.sort_values([\"store_nbr\", \"date\"]), x='date', y='transactions', color='store_nbr',title = \"Transactions\" )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:38.982623Z","iopub.execute_input":"2021-10-30T20:14:38.983057Z","iopub.status.idle":"2021-10-30T20:14:44.42823Z","shell.execute_reply.started":"2021-10-30T20:14:38.983011Z","shell.execute_reply":"2021-10-30T20:14:44.425979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a stable pattern in Transaction. All months are similar except December from 2013 to 2017 by boxplot. In addition, we've just seen same pattern for each store in previous plot. Store sales had always increased at the end of the year.","metadata":{}},{"cell_type":"code","source":"a = transactions.copy()\na[\"year\"] = a.date.dt.year\na[\"month\"] = a.date.dt.month\npx.box(a, x=\"year\", y=\"transactions\" , color = \"month\", title = \"Transactions\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:44.430496Z","iopub.execute_input":"2021-10-30T20:14:44.431065Z","iopub.status.idle":"2021-10-30T20:14:45.135475Z","shell.execute_reply.started":"2021-10-30T20:14:44.431017Z","shell.execute_reply":"2021-10-30T20:14:45.134063Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's take a look at transactions by using monthly average sales!**\n\n We've just learned a pattern what increases sales. It was the end of the year. We can see that transactions increase in spring and decrease after spring.","metadata":{}},{"cell_type":"code","source":"a = transactions.set_index(\"date\").resample(\"M\").transactions.mean().reset_index()\na[\"year\"] = a.date.dt.year\npx.line(a, x='date', y='transactions', color='year',title = \"Monthly Average Transactions\" )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:45.137692Z","iopub.execute_input":"2021-10-30T20:14:45.138634Z","iopub.status.idle":"2021-10-30T20:14:45.311386Z","shell.execute_reply.started":"2021-10-30T20:14:45.138563Z","shell.execute_reply":"2021-10-30T20:14:45.310155Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we look at their relationship, we can see that there is a highly correlation between total sales and transactions also. ","metadata":{}},{"cell_type":"code","source":"px.scatter(temp, x = \"transactions\", y = \"sales\", trendline = \"ols\", trendline_color_override = \"red\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:45.313763Z","iopub.execute_input":"2021-10-30T20:14:45.314266Z","iopub.status.idle":"2021-10-30T20:14:46.358804Z","shell.execute_reply.started":"2021-10-30T20:14:45.314219Z","shell.execute_reply":"2021-10-30T20:14:46.357286Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The days of week is very important for shopping. It shows us a great pattern. Stores make more transactions at weekends. Almost, the patterns are same from 2013 to 2017 and Saturday is the most important day for shopping.","metadata":{}},{"cell_type":"code","source":"a = transactions.copy()\na[\"year\"] = a.date.dt.year\na[\"dayofweek\"] = a.date.dt.dayofweek+1\na = a.groupby([\"year\", \"dayofweek\"]).transactions.mean().reset_index()\npx.line(a, x=\"dayofweek\", y=\"transactions\" , color = \"year\", title = \"Transactions\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:46.360689Z","iopub.execute_input":"2021-10-30T20:14:46.361343Z","iopub.status.idle":"2021-10-30T20:14:46.50822Z","shell.execute_reply.started":"2021-10-30T20:14:46.361293Z","shell.execute_reply":"2021-10-30T20:14:46.507226Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Oil Price\n\n\nThe economy is one of the biggest problem for the governments and people. It affects all of things in a good or bad way. In our case, Ecuador is an oil-dependent country. Changing oil prices in Ecuador will cause a variance in the model. I researched Ecuador's economy to be able to understand much better and I found an article from IMF. You are supposed to read it if you want to make better models by using oil data.\n\n- https://www.imf.org/en/News/Articles/2019/03/20/NA032119-Ecuador-New-Economic-Plan-Explained\n\n<br>\n\n<center><img src=\"https://github.com/EkremBayar/Kaggle/blob/main/Images/imf_sf.PNG?raw=true\" style=\"width:50%;height:10%;\"></center>\n\n<br>\n\nThere are some missing data points in the daily oil data as you can see below. You can treat the data by using various imputation methods. However, I chose a simple solution for that. Linear Interpolation is suitable for this time serie. You can see the trend and predict missing data points, when you look at a time serie plot of oil price.","metadata":{}},{"cell_type":"code","source":"# Import \noil = pd.read_csv(\"../input/store-sales-time-series-forecasting/oil.csv\")\noil[\"date\"] = pd.to_datetime(oil.date)\n# Resample\noil = oil.set_index(\"date\").dcoilwtico.resample(\"D\").sum().reset_index()\n# Interpolate\noil[\"dcoilwtico\"] = np.where(oil[\"dcoilwtico\"] == 0, np.nan, oil[\"dcoilwtico\"])\noil[\"dcoilwtico_interpolated\"] =oil.dcoilwtico.interpolate()\n# Plot\np = oil.melt(id_vars=['date']+list(oil.keys()[5:]), var_name='Legend')\npx.line(p.sort_values([\"Legend\", \"date\"], ascending = [False, True]), x='date', y='value', color='Legend',title = \"Daily Oil Price\" )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:46.510093Z","iopub.execute_input":"2021-10-30T20:14:46.510822Z","iopub.status.idle":"2021-10-30T20:14:46.767464Z","shell.execute_reply.started":"2021-10-30T20:14:46.510751Z","shell.execute_reply":"2021-10-30T20:14:46.766316Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I just said, \"Ecuador is a oil-dependent country\" but is it true? Can we really see that from the data by looking at?**\n\nFirst of all, let's look at the correlations for sales and transactions. The correlation values are not strong but the sign of sales is negative. Maybe, we can catch a clue. Logically, if daily oil price is high, we expect that the Ecuador's economy is bad and it means the price of product increases and sales decreases. There is a negative relationship here.  ","metadata":{}},{"cell_type":"code","source":"temp = pd.merge(temp, oil, how = \"left\")\nprint(\"Correlation with Daily Oil Prices\")\nprint(temp.drop([\"store_nbr\", \"dcoilwtico\"], axis = 1).corr(\"spearman\").dcoilwtico_interpolated.loc[[\"sales\", \"transactions\"]], \"\\n\")\n\n\nfig, axes = plt.subplots(1, 2, figsize = (15,5))\ntemp.plot.scatter(x = \"dcoilwtico_interpolated\", y = \"transactions\", ax=axes[0])\ntemp.plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[1], color = \"r\")\naxes[0].set_title('Daily oil price & Transactions', fontsize = 15)\naxes[1].set_title('Daily Oil Price & Sales', fontsize = 15);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:46.771672Z","iopub.execute_input":"2021-10-30T20:14:46.772343Z","iopub.status.idle":"2021-10-30T20:14:48.011995Z","shell.execute_reply.started":"2021-10-30T20:14:46.772289Z","shell.execute_reply":"2021-10-30T20:14:48.010697Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You should never decide what you will do by looking at a graph or result! You are supposed to change your view and define new hypotheses.\n\nWe would have been wrong if we had looked at some simple outputs just like above and we had said that there is no relationship with oil prices and let's not use oil price data.\n\nAll right! We are aware of analyzing deeply now. Let's draw a scatter plot but let's pay attention for product families this time. All of the plots almost contains same pattern. When daily oil price is under about 70, there are more sales in the data. There are 2 cluster here. They are over 70 and under 70. It seems pretty understandable actually. \n\nWe are in a good way I think. What do you think? Just now, we couldn't see a pattern for daily oil price, but now we extracted a new pattern from it.","metadata":{}},{"cell_type":"code","source":"a = pd.merge(train.groupby([\"date\", \"family\"]).sales.sum().reset_index(), oil.drop(\"dcoilwtico\", axis = 1), how = \"left\")\nc = a.groupby(\"family\").corr(\"spearman\").reset_index()\nc = c[c.level_1 == \"dcoilwtico_interpolated\"][[\"family\", \"sales\"]].sort_values(\"sales\")\n\nfig, axes = plt.subplots(7, 5, figsize = (20,20))\nfor i, fam in enumerate(c.family):\n    if i < 6:\n        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[0, i-1])\n        axes[0, i-1].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n        axes[0, i-1].axvline(x=70, color='r', linestyle='--')\n    if i >= 6 and i<11:\n        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[1, i-6])\n        axes[1, i-6].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n        axes[1, i-6].axvline(x=70, color='r', linestyle='--')\n    if i >= 11 and i<16:\n        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[2, i-11])\n        axes[2, i-11].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n        axes[2, i-11].axvline(x=70, color='r', linestyle='--')\n    if i >= 16 and i<21:\n        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[3, i-16])\n        axes[3, i-16].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n        axes[3, i-16].axvline(x=70, color='r', linestyle='--')\n    if i >= 21 and i<26:\n        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[4, i-21])\n        axes[4, i-21].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n        axes[4, i-21].axvline(x=70, color='r', linestyle='--')\n    if i >= 26 and i < 31:\n        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[5, i-26])\n        axes[5, i-26].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n        axes[5, i-26].axvline(x=70, color='r', linestyle='--')\n    if i >= 31 :\n        a[a.family == fam].plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[6, i-31])\n        axes[6, i-31].set_title(fam+\"\\n Correlation:\"+str(c[c.family == fam].sales.iloc[0])[:6], fontsize = 12)\n        axes[6, i-31].axvline(x=70, color='r', linestyle='--')\n        \n        \nplt.tight_layout(pad=5)\nplt.suptitle(\"Daily Oil Product & Total Family Sales \\n\", fontsize = 20);\nplt.show()\n        ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:48.01439Z","iopub.execute_input":"2021-10-30T20:14:48.014822Z","iopub.status.idle":"2021-10-30T20:14:57.352883Z","shell.execute_reply.started":"2021-10-30T20:14:48.014774Z","shell.execute_reply":"2021-10-30T20:14:57.35177Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Sales\n\nOur main objective is, predicting store sales for each product family. For this reason, sales column should be examined more seriously. We need to learn everthing such as seasonality, trends, anomalies, similarities with other time series and so on.","metadata":{}},{"cell_type":"markdown","source":"Most of the stores are similar to each other, when we examine them with correlation matrix. Some stores, such as 20, 21, 22, and 52 may be a little different.","metadata":{}},{"cell_type":"code","source":"a = train[[\"store_nbr\", \"sales\"]]\na[\"ind\"] = 1\na[\"ind\"] = a.groupby(\"store_nbr\").ind.cumsum().values\na = pd.pivot(a, index = \"ind\", columns = \"store_nbr\", values = \"sales\").corr()\nmask = np.triu(a.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(a,\n        annot=True,\n        fmt='.1f',\n        cmap='coolwarm',\n        square=True,\n        mask=mask,\n        linewidths=1,\n        cbar=False)\nplt.title(\"Correlations among stores\",fontsize = 20)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:14:57.354408Z","iopub.execute_input":"2021-10-30T20:14:57.355719Z","iopub.status.idle":"2021-10-30T20:15:06.726559Z","shell.execute_reply.started":"2021-10-30T20:14:57.355619Z","shell.execute_reply":"2021-10-30T20:15:06.724903Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a graph that shows us daily total sales below.","metadata":{}},{"cell_type":"code","source":"a = train.set_index(\"date\").groupby(\"store_nbr\").resample(\"D\").sales.sum().reset_index()\npx.line(a, x = \"date\", y= \"sales\", color = \"store_nbr\", title = \"Daily total sales of the stores\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:15:06.728833Z","iopub.execute_input":"2021-10-30T20:15:06.729353Z","iopub.status.idle":"2021-10-30T20:15:11.312378Z","shell.execute_reply.started":"2021-10-30T20:15:06.729301Z","shell.execute_reply":"2021-10-30T20:15:11.311083Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I realized some unnecessary rows in the data while I was looking at the time serie of the stores one by one. If you select the stores from above, some of them have no sales at the beginning of 2013. You can see them, if you look at the those stores 20, 21, 22, 29, 36, 42, 52 and 53. I decided to remove those rows before the stores opened. In the following codes, we will get rid of them.","metadata":{}},{"cell_type":"code","source":"print(train.shape)\ntrain = train[~((train.store_nbr == 52) & (train.date < \"2017-04-20\"))]\ntrain = train[~((train.store_nbr == 22) & (train.date < \"2015-10-09\"))]\ntrain = train[~((train.store_nbr == 42) & (train.date < \"2015-08-21\"))]\ntrain = train[~((train.store_nbr == 21) & (train.date < \"2015-07-24\"))]\ntrain = train[~((train.store_nbr == 29) & (train.date < \"2015-03-20\"))]\ntrain = train[~((train.store_nbr == 20) & (train.date < \"2015-02-13\"))]\ntrain = train[~((train.store_nbr == 53) & (train.date < \"2014-05-29\"))]\ntrain = train[~((train.store_nbr == 36) & (train.date < \"2013-05-09\"))]\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-30T20:15:11.314759Z","iopub.execute_input":"2021-10-30T20:15:11.315946Z","iopub.status.idle":"2021-10-30T20:15:12.577661Z","shell.execute_reply.started":"2021-10-30T20:15:11.315885Z","shell.execute_reply":"2021-10-30T20:15:12.576158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Zero Forecasting\n\nSome stores don't sell some product families. In the following code, you can see which products aren't sold in which stores. It isn't difficult to forecast them next 15 days. Their forecasts must be 0 next 15 days.\n\nI will remove them from the data and create a new data frame for product families which never sell. Then, when we are at submission part, I will combine that data frame with our predictions.","metadata":{}},{"cell_type":"code","source":"c = train.groupby([\"store_nbr\", \"family\"]).sales.sum().reset_index().sort_values([\"family\",\"store_nbr\"])\nc = c[c.sales == 0]\nc","metadata":{"execution":{"iopub.status.busy":"2021-10-30T20:15:12.579306Z","iopub.execute_input":"2021-10-30T20:15:12.579673Z","iopub.status.idle":"2021-10-30T20:15:12.979598Z","shell.execute_reply.started":"2021-10-30T20:15:12.579624Z","shell.execute_reply":"2021-10-30T20:15:12.978377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\n# Anti Join\nouter_join = train.merge(c[c.sales == 0].drop(\"sales\",axis = 1), how = 'outer', indicator = True)\ntrain = outer_join[~(outer_join._merge == 'both')].drop('_merge', axis = 1)\ndel outer_join\ngc.collect()\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-30T20:15:12.98156Z","iopub.execute_input":"2021-10-30T20:15:12.981986Z","iopub.status.idle":"2021-10-30T20:15:15.538006Z","shell.execute_reply.started":"2021-10-30T20:15:12.981935Z","shell.execute_reply":"2021-10-30T20:15:15.536348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zero_prediction = []\nfor i in range(0,len(c)):\n    zero_prediction.append(\n        pd.DataFrame({\n            \"date\":pd.date_range(\"2017-08-16\", \"2017-08-31\").tolist(),\n            \"store_nbr\":c.store_nbr.iloc[i],\n            \"family\":c.family.iloc[i],\n            \"sales\":0\n        })\n    )\nzero_prediction = pd.concat(zero_prediction)\ndel c\ngc.collect()\nzero_prediction","metadata":{"execution":{"iopub.status.busy":"2021-10-30T20:15:15.539889Z","iopub.execute_input":"2021-10-30T20:15:15.541068Z","iopub.status.idle":"2021-10-30T20:15:15.923711Z","shell.execute_reply.started":"2021-10-30T20:15:15.540981Z","shell.execute_reply":"2021-10-30T20:15:15.921938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our first forecasting is done! You don't need the machine learning or deep learning or another things for these time series because we had some simple time series.\n\n<center><img src=\"https://preview.redd.it/nc5ua4x8lfg31.png?auto=webp&s=620a441877c6e497f7bc9a4b6c39548bb00aef0a\" style=\"width:50%;height:10%;\"></center>\n","metadata":{}},{"cell_type":"markdown","source":"## Are The Product Families Active or Passive?\n\nSome products can sell rarely in the stores. When I worked on a product supply demand for restuarants project at my previous job, some products were passive if they never bought in the last two months. I want to apply this domain knowledge here and I will look on the last 60 days.\n\nHowever, some product families depends on seasonality. Some of them might not active on the last 60 days but it doesn't mean it is passive.","metadata":{}},{"cell_type":"code","source":"c = train.groupby([\"family\", \"store_nbr\"]).tail(60).groupby([\"family\", \"store_nbr\"]).sales.sum().reset_index()\nc[c.sales == 0]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:15:15.925691Z","iopub.execute_input":"2021-10-30T20:15:15.92617Z","iopub.status.idle":"2021-10-30T20:15:16.428283Z","shell.execute_reply.started":"2021-10-30T20:15:15.926104Z","shell.execute_reply":"2021-10-30T20:15:16.427138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see below, these examples are too rare and also the sales are low. I'm open your suggestions for these families. I won't do anything for now but, you would like to improve your model you can focus on that.\n\nBut still, I want to use that knowledge whether it is simple and I will create a new feature. It shows that the product family is active or not.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,5, figsize = (20,4))\ntrain[(train.store_nbr == 10) & (train.family == \"LAWN AND GARDEN\")].set_index(\"date\").sales.plot(ax = ax[0], title = \"STORE 10 - LAWN AND GARDEN\")\ntrain[(train.store_nbr == 36) & (train.family == \"LADIESWEAR\")].set_index(\"date\").sales.plot(ax = ax[1], title = \"STORE 36 - LADIESWEAR\")\ntrain[(train.store_nbr == 6) & (train.family == \"SCHOOL AND OFFICE SUPPLIES\")].set_index(\"date\").sales.plot(ax = ax[2], title = \"STORE 6 - SCHOOL AND OFFICE SUPPLIES\")\ntrain[(train.store_nbr == 14) & (train.family == \"BABY CARE\")].set_index(\"date\").sales.plot(ax = ax[3], title = \"STORE 14 - BABY CARE\")\ntrain[(train.store_nbr == 53) & (train.family == \"BOOKS\")].set_index(\"date\").sales.plot(ax = ax[4], title = \"STORE 43 - BOOKS\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:15:16.430033Z","iopub.execute_input":"2021-10-30T20:15:16.430896Z","iopub.status.idle":"2021-10-30T20:15:19.943178Z","shell.execute_reply.started":"2021-10-30T20:15:16.430803Z","shell.execute_reply":"2021-10-30T20:15:19.941266Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can catch the trends, seasonality and anomalies for families.","metadata":{}},{"cell_type":"code","source":"a = train.set_index(\"date\").groupby(\"family\").resample(\"D\").sales.sum().reset_index()\npx.line(a, x = \"date\", y= \"sales\", color = \"family\", title = \"Daily total sales of the family\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:15:19.945182Z","iopub.execute_input":"2021-10-30T20:15:19.946264Z","iopub.status.idle":"2021-10-30T20:15:23.782224Z","shell.execute_reply.started":"2021-10-30T20:15:19.946198Z","shell.execute_reply":"2021-10-30T20:15:23.780422Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are working with the stores. Well, there are plenty of products in the stores and we need to know which product family sells much more? Let's make a barplot to see that.\n\nThe graph shows us GROCERY I and BEVERAGES are the top selling families.","metadata":{}},{"cell_type":"code","source":"a = train.groupby(\"family\").sales.mean().sort_values(ascending = False).reset_index()\npx.bar(a, y = \"family\", x=\"sales\", color = \"family\", title = \"Which product family preferred more?\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:15:23.784539Z","iopub.execute_input":"2021-10-30T20:15:23.785949Z","iopub.status.idle":"2021-10-30T20:15:24.388342Z","shell.execute_reply.started":"2021-10-30T20:15:23.785775Z","shell.execute_reply":"2021-10-30T20:15:24.387465Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Does onpromotion column cause a data leakage problem?\n\nIt is really a good question. The Data Leakage is one of the biggest problem when we will fit a model. There is a great discussion from [Nesterenko Marina](https://www.kaggle.com/nesterenkomarina) [@nesterenkomarina](https://www.kaggle.com/nesterenkomarina). You should look at it before fitting a model.\n\n- https://www.kaggle.com/c/store-sales-time-series-forecasting/discussion/277067","metadata":{}},{"cell_type":"code","source":"print(\"Spearman Correlation between Sales and Onpromotion: {:,.4f}\".format(train.corr(\"spearman\").sales.loc[\"onpromotion\"]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:15:24.389899Z","iopub.execute_input":"2021-10-30T20:15:24.390985Z","iopub.status.idle":"2021-10-30T20:15:25.76809Z","shell.execute_reply.started":"2021-10-30T20:15:24.390923Z","shell.execute_reply":"2021-10-30T20:15:25.766948Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<center><img src=\"https://github.com/EkremBayar/Kaggle/blob/main/Images/kr.PNG?raw=true\n\" style=\"width:90%;height:10%;\"></center>","metadata":{}},{"cell_type":"markdown","source":"How different can stores be from each other? I couldn't find a major pattern among the stores actually. But I only looked at a single plot. There may be some latent patterns. ","metadata":{}},{"cell_type":"code","source":"d = pd.merge(train, stores)\nd[\"store_nbr\"] = d[\"store_nbr\"].astype(\"int8\")\nd[\"year\"] = d.date.dt.year\npx.line(d.groupby([\"city\", \"year\"]).sales.mean().reset_index(), x = \"year\", y = \"sales\", color = \"city\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:15:25.769792Z","iopub.execute_input":"2021-10-30T20:15:25.771009Z","iopub.status.idle":"2021-10-30T20:15:26.987281Z","shell.execute_reply.started":"2021-10-30T20:15:25.770935Z","shell.execute_reply":"2021-10-30T20:15:26.98605Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Holidays and Events\n\nWhat a mess! Probably, you are confused due to the holidays and events data. It contains a lot of information inside but, don't worry. You just need to take a breathe and think! It is a meta-data so you have to split it logically and make the data useful.\n\nWhat are our problems?\n- Some national holidays have been transferred.\n- There might be a few holidays in one day. When we merged all of data, number of rows might increase. We don't want duplicates.\n- What is the scope of holidays? It can be regional or national or local. You need to split them by the scope.\n- Work day issue\n- Some specific events\n- Creating new features etc.\n\n\nEnd of the section, they won't be a problem anymore!","metadata":{}},{"cell_type":"code","source":"holidays = pd.read_csv(\"../input/store-sales-time-series-forecasting/holidays_events.csv\")\nholidays[\"date\"] = pd.to_datetime(holidays.date)\n\n# holidays[holidays.type == \"Holiday\"]\n# holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)]\n\n# Transferred Holidays\ntr1 = holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)].drop(\"transferred\", axis = 1).reset_index(drop = True)\ntr2 = holidays[(holidays.type == \"Transfer\")].drop(\"transferred\", axis = 1).reset_index(drop = True)\ntr = pd.concat([tr1,tr2], axis = 1)\ntr = tr.iloc[:, [5,1,2,3,4]]\n\nholidays = holidays[(holidays.transferred == False) & (holidays.type != \"Transfer\")].drop(\"transferred\", axis = 1)\nholidays = holidays.append(tr).reset_index(drop = True)\n\n\n# Additional Holidays\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Additional\", \"Holiday\", holidays[\"type\"])\n\n# Bridge Holidays\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"Puente \", \"\")\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Bridge\", \"Holiday\", holidays[\"type\"])\n\n \n# Work Day Holidays, that is meant to payback the Bridge.\nwork_day = holidays[holidays.type == \"Work Day\"]  \nholidays = holidays[holidays.type != \"Work Day\"]  \n\n\n# Split\n\n# Events are national\nevents = holidays[holidays.type == \"Event\"].drop([\"type\", \"locale\", \"locale_name\"], axis = 1).rename({\"description\":\"events\"}, axis = 1)\n\nholidays = holidays[holidays.type != \"Event\"].drop(\"type\", axis = 1)\nregional = holidays[holidays.locale == \"Regional\"].rename({\"locale_name\":\"state\", \"description\":\"holiday_regional\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\nnational = holidays[holidays.locale == \"National\"].rename({\"description\":\"holiday_national\"}, axis = 1).drop([\"locale\", \"locale_name\"], axis = 1).drop_duplicates()\nlocal = holidays[holidays.locale == \"Local\"].rename({\"description\":\"holiday_local\", \"locale_name\":\"city\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\n\n\n\nd = pd.merge(train.append(test), stores)\nd[\"store_nbr\"] = d[\"store_nbr\"].astype(\"int8\")\n\n\n# National Holidays & Events\n#d = pd.merge(d, events, how = \"left\")\nd = pd.merge(d, national, how = \"left\")\n# Regional\nd = pd.merge(d, regional, how = \"left\", on = [\"date\", \"state\"])\n# Local\nd = pd.merge(d, local, how = \"left\", on = [\"date\", \"city\"])\n\n# Work Day: It will be removed when real work day colum created\nd = pd.merge(d,  work_day[[\"date\", \"type\"]].rename({\"type\":\"IsWorkDay\"}, axis = 1),how = \"left\")\n\n# EVENTS\nevents[\"events\"] =np.where(events.events.str.contains(\"futbol\"), \"Futbol\", events.events)\n\ndef one_hot_encoder(df, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = df.select_dtypes([\"category\", \"object\"]).columns.tolist()\n    # categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    df.columns = df.columns.str.replace(\" \", \"_\")\n    return df, df.columns.tolist()\n\nevents, events_cat = one_hot_encoder(events, nan_as_category=False)\nevents[\"events_Dia_de_la_Madre\"] = np.where(events.date == \"2016-05-08\", 1,events[\"events_Dia_de_la_Madre\"])\nevents = events.drop(239)\n\nd = pd.merge(d, events, how = \"left\")\nd[events_cat] = d[events_cat].fillna(0)\n\n# New features\nd[\"holiday_national_binary\"] = np.where(d.holiday_national.notnull(), 1, 0)\nd[\"holiday_local_binary\"] = np.where(d.holiday_local.notnull(), 1, 0)\nd[\"holiday_regional_binary\"] = np.where(d.holiday_regional.notnull(), 1, 0)\n\n# \nd[\"national_independence\"] = np.where(d.holiday_national.isin(['Batalla de Pichincha',  'Independencia de Cuenca', 'Independencia de Guayaquil', 'Independencia de Guayaquil', 'Primer Grito de Independencia']), 1, 0)\nd[\"local_cantonizacio\"] = np.where(d.holiday_local.str.contains(\"Cantonizacio\"), 1, 0)\nd[\"local_fundacion\"] = np.where(d.holiday_local.str.contains(\"Fundacion\"), 1, 0)\nd[\"local_independencia\"] = np.where(d.holiday_local.str.contains(\"Independencia\"), 1, 0)\n\n\nholidays, holidays_cat = one_hot_encoder(d[[\"holiday_national\",\"holiday_regional\",\"holiday_local\"]], nan_as_category=False)\nd = pd.concat([d.drop([\"holiday_national\",\"holiday_regional\",\"holiday_local\"], axis = 1),holidays], axis = 1)\n\nhe_cols = d.columns[d.columns.str.startswith(\"events\")].tolist() + d.columns[d.columns.str.startswith(\"holiday\")].tolist() + d.columns[d.columns.str.startswith(\"national\")].tolist()+ d.columns[d.columns.str.startswith(\"local\")].tolist()\nd[he_cols] = d[he_cols].astype(\"int8\")\n\nd[[\"family\", \"city\", \"state\", \"type\"]] = d[[\"family\", \"city\", \"state\", \"type\"]].astype(\"category\")\n\ndel holidays, holidays_cat, work_day, local, regional, national, events, events_cat, tr, tr1, tr2, he_cols\ngc.collect()\n\nd.head(10)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:15:26.98932Z","iopub.execute_input":"2021-10-30T20:15:26.989672Z","iopub.status.idle":"2021-10-30T20:16:07.457357Z","shell.execute_reply.started":"2021-10-30T20:15:26.989637Z","shell.execute_reply":"2021-10-30T20:16:07.45602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's apply an AB test to Events and Holidays features. Are they statistically significant? Also it can be a good way for first feature selection.**\n\n- *H0: The sales are equal* **(M1 = M2)**\n- *H1: The sales are not equal* **(M1 != M2)**","metadata":{}},{"cell_type":"code","source":"def AB_Test(dataframe, group, target):\n    \n    # Packages\n    from scipy.stats import shapiro\n    import scipy.stats as stats\n    \n    # Split A/B\n    groupA = dataframe[dataframe[group] == 1][target]\n    groupB = dataframe[dataframe[group] == 0][target]\n    \n    # Assumption: Normality\n    ntA = shapiro(groupA)[1] < 0.05\n    ntB = shapiro(groupB)[1] < 0.05\n    # H0: Distribution is Normal! - False\n    # H1: Distribution is not Normal! - True\n    \n    if (ntA == False) & (ntB == False): # \"H0: Normal Distribution\"\n        # Parametric Test\n        # Assumption: Homogeneity of variances\n        leveneTest = stats.levene(groupA, groupB)[1] < 0.05\n        # H0: Homogeneity: False\n        # H1: Heterogeneous: True\n        \n        if leveneTest == False:\n            # Homogeneity\n            ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n            # H0: M1 == M2 - False\n            # H1: M1 != M2 - True\n        else:\n            # Heterogeneous\n            ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]\n            # H0: M1 == M2 - False\n            # H1: M1 != M2 - True\n    else:\n        # Non-Parametric Test\n        ttest = stats.mannwhitneyu(groupA, groupB)[1] \n        # H0: M1 == M2 - False\n        # H1: M1 != M2 - True\n        \n    # Result\n    temp = pd.DataFrame({\n        \"AB Hypothesis\":[ttest < 0.05], \n        \"p-value\":[ttest]\n    })\n    temp[\"Test Type\"] = np.where((ntA == False) & (ntB == False), \"Parametric\", \"Non-Parametric\")\n    temp[\"AB Hypothesis\"] = np.where(temp[\"AB Hypothesis\"] == False, \"Fail to Reject H0\", \"Reject H0\")\n    temp[\"Comment\"] = np.where(temp[\"AB Hypothesis\"] == \"Fail to Reject H0\", \"A/B groups are similar!\", \"A/B groups are not similar!\")\n    temp[\"Feature\"] = group\n    temp[\"GroupA_mean\"] = groupA.mean()\n    temp[\"GroupB_mean\"] = groupB.mean()\n    temp[\"GroupA_median\"] = groupA.median()\n    temp[\"GroupB_median\"] = groupB.median()\n    \n    # Columns\n    if (ntA == False) & (ntB == False):\n        temp[\"Homogeneity\"] = np.where(leveneTest == False, \"Yes\", \"No\")\n        temp = temp[[\"Feature\",\"Test Type\", \"Homogeneity\",\"AB Hypothesis\", \"p-value\", \"Comment\", \"GroupA_mean\", \"GroupB_mean\", \"GroupA_median\", \"GroupB_median\"]]\n    else:\n        temp = temp[[\"Feature\",\"Test Type\",\"AB Hypothesis\", \"p-value\", \"Comment\", \"GroupA_mean\", \"GroupB_mean\", \"GroupA_median\", \"GroupB_median\"]]\n    \n    # Print Hypothesis\n    # print(\"# A/B Testing Hypothesis\")\n    # print(\"H0: A == B\")\n    # print(\"H1: A != B\", \"\\n\")\n    \n    return temp\n    \n# Apply A/B Testing\nhe_cols = d.columns[d.columns.str.startswith(\"events\")].tolist() + d.columns[d.columns.str.startswith(\"holiday\")].tolist() + d.columns[d.columns.str.startswith(\"national\")].tolist()+ d.columns[d.columns.str.startswith(\"local\")].tolist()\nab = []\nfor i in he_cols:\n    ab.append(AB_Test(dataframe=d[d.sales.notnull()], group = i, target = \"sales\"))\nab = pd.concat(ab)\nab","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:16:07.459468Z","iopub.execute_input":"2021-10-30T20:16:07.459869Z","iopub.status.idle":"2021-10-30T20:21:02.47364Z","shell.execute_reply.started":"2021-10-30T20:16:07.459801Z","shell.execute_reply":"2021-10-30T20:21:02.472285Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d.groupby([\"family\",\"events_Futbol\"]).sales.mean()[:60]","metadata":{"execution":{"iopub.status.busy":"2021-10-30T20:21:02.475942Z","iopub.execute_input":"2021-10-30T20:21:02.476429Z","iopub.status.idle":"2021-10-30T20:21:02.619237Z","shell.execute_reply.started":"2021-10-30T20:21:02.476371Z","shell.execute_reply":"2021-10-30T20:21:02.617624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Time Related Features\n\nHow many features can you create from only date column? I'm sharing an example of time related features. You can expand the features with your imagination or your needs. ","metadata":{"execution":{"iopub.status.busy":"2021-10-22T21:09:35.342168Z","iopub.execute_input":"2021-10-22T21:09:35.343102Z","iopub.status.idle":"2021-10-22T21:09:37.469635Z","shell.execute_reply.started":"2021-10-22T21:09:35.343054Z","shell.execute_reply":"2021-10-22T21:09:37.468689Z"}}},{"cell_type":"code","source":"# Time Related Features\ndef create_date_features(df):\n    df['month'] = df.date.dt.month.astype(\"int8\")\n    df['day_of_month'] = df.date.dt.day.astype(\"int8\")\n    df['day_of_year'] = df.date.dt.dayofyear.astype(\"int16\")\n    df['week_of_month'] = (df.date.apply(lambda d: (d.day-1) // 7 + 1)).astype(\"int8\")\n    df['week_of_year'] = (df.date.dt.weekofyear).astype(\"int8\")\n    df['day_of_week'] = (df.date.dt.dayofweek + 1).astype(\"int8\")\n    df['year'] = df.date.dt.year.astype(\"int32\")\n    df[\"is_wknd\"] = (df.date.dt.weekday // 4).astype(\"int8\")\n    df[\"quarter\"] = df.date.dt.quarter.astype(\"int8\")\n    df['is_month_start'] = df.date.dt.is_month_start.astype(\"int8\")\n    df['is_month_end'] = df.date.dt.is_month_end.astype(\"int8\")\n    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(\"int8\")\n    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(\"int8\")\n    df['is_year_start'] = df.date.dt.is_year_start.astype(\"int8\")\n    df['is_year_end'] = df.date.dt.is_year_end.astype(\"int8\")\n    # 0: Winter - 1: Spring - 2: Summer - 3: Fall\n    df[\"season\"] = np.where(df.month.isin([12,1,2]), 0, 1)\n    df[\"season\"] = np.where(df.month.isin([6,7,8]), 2, df[\"season\"])\n    df[\"season\"] = pd.Series(np.where(df.month.isin([9, 10, 11]), 3, df[\"season\"])).astype(\"int8\")\n    return df\nd = create_date_features(d)\n\n\n\n\n# Workday column\nd[\"workday\"] = np.where((d.holiday_national_binary == 1) | (d.holiday_local_binary==1) | (d.holiday_regional_binary==1) | (d['day_of_week'].isin([6,7])), 0, 1)\nd[\"workday\"] = pd.Series(np.where(d.IsWorkDay.notnull(), 1, d[\"workday\"])).astype(\"int8\")\nd.drop(\"IsWorkDay\", axis = 1, inplace = True)\n\n# Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. \n# Supermarket sales could be affected by this.\nd[\"wageday\"] = pd.Series(np.where((d['is_month_end'] == 1) | (d[\"day_of_month\"] == 15), 1, 0)).astype(\"int8\")\n\nd.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T20:21:02.621299Z","iopub.execute_input":"2021-10-30T20:21:02.621663Z","iopub.status.idle":"2021-10-30T20:21:29.123113Z","shell.execute_reply.started":"2021-10-30T20:21:02.621626Z","shell.execute_reply":"2021-10-30T20:21:29.121434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Did Earhquake affect the store sales?\n\nA magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.\n\nComparing average sales by year, month and product family will be one of the best ways to be able to understand how earthquake had affected the store sales.\n\nWe can use the data of March, April, May and June and there may be increasing or decrasing sales for some product families.\n\nLastly, we extracted a column for earthquake from Holidays and Events data. **\"events_Terremoto_Manabi\"** column will help to fit a better model.","metadata":{}},{"cell_type":"code","source":"d[(d.month.isin([4,5]))].groupby([\"year\"]).sales.mean()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-10-30T20:21:29.130944Z","iopub.execute_input":"2021-10-30T20:21:29.131431Z","iopub.status.idle":"2021-10-30T20:21:29.428313Z","shell.execute_reply.started":"2021-10-30T20:21:29.131377Z","shell.execute_reply":"2021-10-30T20:21:29.426882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### March","metadata":{}},{"cell_type":"code","source":"pd.pivot_table(d[(d.month.isin([3]))], index=\"year\", columns=\"family\", values=\"sales\", aggfunc=\"mean\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:21:29.43046Z","iopub.execute_input":"2021-10-30T20:21:29.431293Z","iopub.status.idle":"2021-10-30T20:21:29.569361Z","shell.execute_reply.started":"2021-10-30T20:21:29.43124Z","shell.execute_reply":"2021-10-30T20:21:29.568099Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### April - May","metadata":{}},{"cell_type":"code","source":"pd.pivot_table(d[(d.month.isin([4,5]))], index=\"year\", columns=\"family\", values=\"sales\", aggfunc=\"mean\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:21:29.571119Z","iopub.execute_input":"2021-10-30T20:21:29.571659Z","iopub.status.idle":"2021-10-30T20:21:29.767033Z","shell.execute_reply.started":"2021-10-30T20:21:29.571606Z","shell.execute_reply":"2021-10-30T20:21:29.765994Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### June","metadata":{}},{"cell_type":"code","source":"pd.pivot_table(d[(d.month.isin([6]))], index=\"year\", columns=\"family\", values=\"sales\", aggfunc=\"mean\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:21:29.768501Z","iopub.execute_input":"2021-10-30T20:21:29.769774Z","iopub.status.idle":"2021-10-30T20:21:29.906917Z","shell.execute_reply.started":"2021-10-30T20:21:29.769706Z","shell.execute_reply":"2021-10-30T20:21:29.905993Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. ACF & PACF for each family\n\nThe lag features means,shifting a time serie forward one step or more than one. So, a lag feature can use in the model to improve it. However, how many lag features should be inside the model? For understanding that, we can use ACF and PACF. The PACF is very useful to decide which features should select.\n\nIn our problem, we have multiple time series and each time series have different pattern of course. You know that those time series consists of store-product family combinations and we have 54 stores and 33 product families. We can't examine all of them one by one. For this reason, I will look at average sales for each product but it will be store independent.\n\nIn addition, the test data contains 15 days for each family. We should be careful when selecting lag features. We can't create new lag features from 1 lag to 15 lag. It must be starting 16. ","metadata":{}},{"cell_type":"code","source":"#a = d[d[\"store_nbr\"]==1].set_index(\"date\")\na = d[(d.sales.notnull())].groupby([\"date\", \"family\"]).sales.mean().reset_index().set_index(\"date\")\nfor num, i in enumerate(a.family.unique()):\n    try:\n        fig, ax = plt.subplots(1,2,figsize=(15,5))\n        temp = a[(a.family == i)]#& (a.sales.notnull())\n        sm.graphics.tsa.plot_acf(temp.sales, lags=365, ax=ax[0], title = \"AUTOCORRELATION\\n\" + i)\n        sm.graphics.tsa.plot_pacf(temp.sales, lags=365, ax=ax[1], title = \"PARTIAL AUTOCORRELATION\\n\" + i)\n    except:\n        pass","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:21:29.908331Z","iopub.execute_input":"2021-10-30T20:21:29.909243Z","iopub.status.idle":"2021-10-30T20:22:57.302892Z","shell.execute_reply.started":"2021-10-30T20:21:29.909179Z","shell.execute_reply":"2021-10-30T20:22:57.301216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I decided to chose these lags 16, 20, 30, 45, 365, 730 from PACF. I don't know that they will help me to improve the model but especially, 365th and 730th lags may be helpful. If you compare 2016 and 2017 years for sales, you can see that they are highly correlated.","metadata":{}},{"cell_type":"code","source":"a = d[d.year.isin([2016,2017])].groupby([\"year\", \"day_of_year\"]).sales.mean().reset_index()\npx.line(a, x = \"day_of_year\", y = \"sales\", color = \"year\", title = \"Average sales for 2016 and 2017\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:22:57.305706Z","iopub.execute_input":"2021-10-30T20:22:57.306224Z","iopub.status.idle":"2021-10-30T20:22:57.699083Z","shell.execute_reply.started":"2021-10-30T20:22:57.306164Z","shell.execute_reply":"2021-10-30T20:22:57.697137Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Simple Moving Average","metadata":{}},{"cell_type":"code","source":"a = train.sort_values([\"store_nbr\", \"family\", \"date\"])\nfor i in [20, 30, 45, 60, 90, 120, 365, 730]:\n    a[\"SMA\"+str(i)+\"_sales_lag16\"] = a.groupby([\"store_nbr\", \"family\"]).rolling(i).sales.mean().shift(16).values\n    a[\"SMA\"+str(i)+\"_sales_lag30\"] = a.groupby([\"store_nbr\", \"family\"]).rolling(i).sales.mean().shift(30).values\n    a[\"SMA\"+str(i)+\"_sales_lag60\"] = a.groupby([\"store_nbr\", \"family\"]).rolling(i).sales.mean().shift(60).values\nprint(\"Correlation\")\na[[\"sales\"]+a.columns[a.columns.str.startswith(\"SMA\")].tolist()].corr()","metadata":{"execution":{"iopub.status.busy":"2021-10-30T20:49:29.987035Z","iopub.execute_input":"2021-10-30T20:49:29.987464Z","iopub.status.idle":"2021-10-30T20:50:14.745448Z","shell.execute_reply.started":"2021-10-30T20:49:29.987433Z","shell.execute_reply":"2021-10-30T20:50:14.744661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = a[(a.store_nbr == 1)].set_index(\"date\")\nfor i in b.family.unique():\n    fig, ax = plt.subplots(2,4,figsize=(20,10))\n    b[b.family == i][[\"sales\", \"SMA20_sales_lag16\"]].plot(legend = True, ax = ax[0,0], linewidth = 4)\n    b[b.family == i][[\"sales\", \"SMA30_sales_lag16\"]].plot(legend = True, ax = ax[0,1], linewidth = 4)\n    b[b.family == i][[\"sales\", \"SMA45_sales_lag16\"]].plot(legend = True, ax = ax[0,2], linewidth = 4)\n    b[b.family == i][[\"sales\", \"SMA60_sales_lag16\"]].plot(legend = True, ax = ax[0,3], linewidth = 4)\n    b[b.family == i][[\"sales\", \"SMA90_sales_lag16\"]].plot(legend = True, ax = ax[1,0], linewidth = 4)\n    b[b.family == i][[\"sales\", \"SMA120_sales_lag16\"]].plot(legend = True, ax = ax[1,1], linewidth = 4)\n    b[b.family == i][[\"sales\", \"SMA365_sales_lag16\"]].plot(legend = True, ax = ax[1,2], linewidth = 4)\n    b[b.family == i][[\"sales\", \"SMA730_sales_lag16\"]].plot(legend = True, ax = ax[1,3], linewidth = 4)\n    plt.suptitle(\"STORE 1 - \"+i, fontsize = 15)\n    plt.tight_layout(pad = 1.5)\n    for j in range(0,4):\n        ax[0,j].legend(fontsize=\"x-large\")\n        ax[1,j].legend(fontsize=\"x-large\")\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:47:57.831881Z","iopub.execute_input":"2021-10-30T20:47:57.832825Z","iopub.status.idle":"2021-10-30T20:48:55.444548Z","shell.execute_reply.started":"2021-10-30T20:47:57.832785Z","shell.execute_reply":"2021-10-30T20:48:55.440069Z"},"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Exponential Moving Average","metadata":{}},{"cell_type":"code","source":"def ewm_features(dataframe, alphas, lags):\n    dataframe = dataframe.copy()\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store_nbr\", \"family\"])['sales']. \\\n                    transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [16, 30, 60, 90]\n\na = ewm_features(a, alphas, lags)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T20:23:13.468724Z","iopub.execute_input":"2021-10-30T20:23:13.469234Z","iopub.status.idle":"2021-10-30T20:24:02.437263Z","shell.execute_reply.started":"2021-10-30T20:23:13.469172Z","shell.execute_reply":"2021-10-30T20:24:02.435357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a[(a.store_nbr == 1) & (a.family == \"GROCERY I\")].set_index(\"date\")[[\"sales\", \"sales_ewm_alpha_095_lag_16\"]].plot(title = \"STORE 1 - GROCERY I\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-30T20:33:04.917076Z","iopub.execute_input":"2021-10-30T20:33:04.917505Z","iopub.status.idle":"2021-10-30T20:33:05.801085Z","shell.execute_reply.started":"2021-10-30T20:33:04.917469Z","shell.execute_reply":"2021-10-30T20:33:05.799392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n- https://www.corporacionfavorita.com/en/\n- https://www.imf.org/en/News/Articles/2019/03/20/NA032119-Ecuador-New-Economic-Plan-Explained\n- https://www.kaggle.com/kashishrastogi/store-sales-forecasting\n- https://rpubs.com/yongks/favorita\n- https://towardsdatascience.com/an-algorithm-to-find-the-best-moving-average-for-stock-trading-1b024672299c","metadata":{}},{"cell_type":"markdown","source":"# NOTEBOOK WILL BE UPDATED AS SOON AS POSSIBLE! \n### Don't forget give an upvote, if you liked it :)","metadata":{}},{"cell_type":"markdown","source":"what should do if Partial Autocorrelation is greater than +1 or less than -1?\n- https://github.com/statsmodels/statsmodels/issues/7179","metadata":{}},{"cell_type":"code","source":"# PASSIVE OR ACTIVE\n# c = train.set_index(\"date\").groupby([\"store_nbr\", \"family\"]).rolling(20).sales.sum().reset_index()\n# c[\"ActivePassive\"] = np.where(c.sales > 0 , 1, 0)\n# c[\"ActivePassive\"] = np.where(c.sales.isnull() , np.nan, c[\"ActivePassive\"])\n# c[\"ActivePassive\"] = c.groupby([\"store_nbr\", \"family\"]).ActivePassive.shift(1).values","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]}]}